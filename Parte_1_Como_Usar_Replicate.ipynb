{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Taller: **Construye un asistente de IA con Streamlit y Replicate**\n",
        "\n",
        "*Rodrigo Oliver*  \n",
        "*Lead Instructor Bootcamp Data Science Online*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introducción\n",
        "\n",
        "En este taller aprenderemos a crear un chatbot utilizando:\n",
        "- GitHub como plataforma de desarrollo\n",
        "- Replicate como proveedor de modelos de lenguaje\n",
        "- Streamlit como soporte de nuestra interfaz\n",
        "\n",
        "### Disclaimer\n",
        "\n",
        "El objetivo del taller es construir una POC o Prueba de Concepto sencilla en la que crearemos una interfaz personalizada para un modelo open-source de IA ya entrenado y disponible en internet.  \n",
        "\n",
        "**Importante, esta implementación NO está lista para producción**, y su propósito es demostrar conceptos fundamentales y proporcionar una base sólida para futuras iteraciones más robustas.\n",
        "\n",
        "### Requisitos previos\n",
        "- Una cuenta en [GitHub](https://github.com/signup)\n",
        "\n",
        "### Instalación\n",
        "\n",
        "Utiliza este repositorio como **plantilla**, esto copiará los archivos en tu perfil de GitHub.  \n",
        "\n",
        "Una vez tengas el repositorio en tu perfil, lo abriremos mediante Codespaces de GitHub que servirá como nuestra plataforma de desarrollo.\n",
        "GitHub te permite utilizar Codespaces de manera gratuita durante:\n",
        "- 120 horas al mes en instancias de 2 núcleos\n",
        "- 60 horas al mes en instancias de 4 núcleos\n",
        "\n",
        "Si lo prefieres, y sabes cómo hacerlo, también puedes descargarlo en tu equipo, para lo que te recomendamos que crees un entorno virtual e instales las dependencias del proyecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Cómo usar Replicate  \n",
        "\n",
        "Adaptado y actualizado de *\"How to use Llama 2\"* por Chanin Nantasenamat [https://blog.streamlit.io/author/chanin/](https://blog.streamlit.io/author/chanin/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **¿Qué es Replicate?**\n",
        "\n",
        "Replicate es una plataforma que facilita el uso de modelos de inteligencia artificial a través de APIs sencillas. Permite ejecutar modelos de código abierto en la nube sin necesidad de configurar infraestructura compleja o gestionar recursos computacionales.  \n",
        "\n",
        "### **¿Por qué Replicate?**\n",
        "\n",
        "Replicate ofrece acceso a cientos de modelos de IA (incluyendo LLMs como Llama 3, Llama 4, Claude, así como modelos de visión, generación de imágenes y audio, etc.) ocupándose totalmente de la infraestructura necesaria para ello.\n",
        "\n",
        "Esto es **extremadamente práctico**, ya que elimina la necesidad de configurar GPUs y servidores muy potentes y caros, proporcionando a cambio un sistema de pago por uso adaptado a cada modelo.\n",
        "\n",
        "Para ello, Replicate nos proporciona una interfaz unificada y simple para interactuar con diversos modelos mediante APIs sencillas, disponibles en diferentes lenguajes.\n",
        "\n",
        "[**Si no sabes qué es una API, pulsa aquí**](https://aws.amazon.com/es/what-is/api/)\n",
        "\n",
        "### **¿Cómo funciona Replicate?**\n",
        "\n",
        "De forma muy resumida:\n",
        "\n",
        "1. Exploras el catálogo de modelos disponibles y eliges el que mejor se adapte a tu caso de uso.  \n",
        "2. Incorporas llamadas a la API de Replicate en tu aplicación con unas pocas líneas de código.  \n",
        "3. Replicate ejecuta el modelo en sus servidores, gestionando toda la infraestructura necesaria.  \n",
        "4. Obtienes las respuestas generadas en tiempo real, con opciones para streaming en muchos modelos.  \n",
        "\n",
        "\n",
        "Para comenzar con Replicate solo necesitas:\n",
        "- Crear una cuenta en [replicate.com](https://replicate.com)\n",
        "- Generar tu API token personal\n",
        "- Instalar la biblioteca de Python en tu entorno local o virtual (`pip install replicate`)\n",
        "- Empezar a hacer llamadas a los modelos desde tu código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **¿Cómo ejecutar Replicate con Python?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIf3Q7QaK4gn"
      },
      "source": [
        "#### **Paso 1: Instalar la librería de Replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install replicate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBzUTg9NMdh"
      },
      "source": [
        "#### **Paso 2: Importar Replicate y fijar el API token en nuestro entorno**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ga2m-1FNP7o"
      },
      "outputs": [],
      "source": [
        "import replicate\n",
        "import os\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"tu_api_key_de_replicate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Paso 3: Documentarnos sobre nuestro modelo**\n",
        "\n",
        "Para este taller hemos elegido el modelo Llama 3-8b-instruct de Meta por su equilibrio entre coste y rendimiento, pese a no tratarse del modelo más potente ni más vanguardista.\n",
        "\n",
        "Llama 3-8b-instruct es lo que se conoce como un *Instruction-Tuned LLM*, es decir, se basa en un modelo fundacional al que se ha sometido a técnicas de ajuste para entender y seguir instrucciones específicas.\n",
        "\n",
        "Toda la información del modelo está disponible en su página oficial de Replicate: [replicate.com/meta/meta-llama-3-8b-instruct](https://replicate.com/meta/meta-llama-3-8b-instruct)\n",
        "\n",
        "##### ¿Qué información encontrarás?\n",
        "\n",
        "- Detalles técnicos de este modelo de 8 billones de parámetros optimizado para chat.\n",
        "- Documentación sobre su arquitectura, entrenamiento y capacidades.\n",
        "- Ejemplos de uso con código de muestra para integrar el modelo en tus aplicaciones.\n",
        "- **Playground interactivo** para probar el modelo directamente en el navegador.\n",
        "- Información sobre el sistema de facturación por tokens (entrada/salida).\n",
        "- Limitaciones y consideraciones éticas, guías para un uso responsable.\n",
        "\n",
        "##### Precios (importante)\n",
        "\n",
        "El modelo elegido tiene un sistema de precios predecible basado en:\n",
        "- Cantidad de tokens de entrada enviados\n",
        "- Cantidad de tokens de salida generados\n",
        "\n",
        "Esta estructura de precios hace que el coste sea más predecible que los modelos facturados por tiempo de computación.\n",
        "\n",
        "[**Si no sabes qué es un token, pulsa aquí**](https://learn.microsoft.com/es-es/dotnet/ai/conceptual/understanding-tokens)\n",
        "\n",
        "##### **Pero si el modelo es open-source, ¿por qué tengo que pagar por usarlo?**\n",
        "\n",
        "**Por el coste de la infraestructura necesaria para ejecutarlo.**    \n",
        "\n",
        "Es cierto que los modelos open-source están disponibles para su descarga en plataformas como [HuggingFace](https://huggingface.co/), PERO para ejecutar un modelo es necesario tener una tarjeta GPU (¡o varias!) con memoria suficiente para instanciar el modelo completo (y eso sin contar si queremos ajustarlo mediante fine-tuning). \n",
        "\n",
        "Además del coste del hardware, también está la complejidad de configurar el entorno, gestionar actualizaciones, escalado automático en caso de desplegar en cloud, etc. Replicate se encarga de todo esto por ti.\n",
        "\n",
        "Existen alternativas a Replicate, por ejemplo HuggingFace también ofrece sus propios proveedores de inferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901Hxea9K7ME"
      },
      "source": [
        "#### **Paso 4: Ejecutar el modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Métodos de ejecución del modelo Llama 3-8b-instruct\n",
        "\n",
        "Como seguro ya sabéis, la forma en la que nos comunicamos con los modelos de lenguaje es mediante los famosos **prompts**, pero en esta ocasión lo haremos a través de la API usando los métodos de su librería de Python.\n",
        "\n",
        "Para el caso de Llama 3-8b-instruct, tenemos tres métodos disponibles para obtener las predicciones:\n",
        "\n",
        "##### 1. Método `run` (más sencillo)\n",
        "\n",
        "- **Funcionamiento**: Envía una solicitud y espera hasta recibir la respuesta completa.\n",
        "- **Ventajas**: Simple de implementar, ideal para scripts y procesamientos por lotes.\n",
        "- **Desventajas**: Bloqueante, tu aplicación debe esperar hasta que se complete toda la generación.\n",
        "- **Ideal para**: Tareas donde necesitas el resultado completo antes de continuar o aplicaciones con procesamiento asíncrono propio.  \n",
        "\n",
        "```python\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        ")\n",
        "print(\"\".join(output))\n",
        "```\n",
        "\n",
        "##### 2. Método `stream` (el que nos interesa)\n",
        "\n",
        "- **Funcionamiento**: Recibe las respuestas del modelo fragmento a fragmento en tiempo real.\n",
        "- **Ventajas**: Proporciona feedback inmediato al usuario, mejora la experiencia de interacción.\n",
        "- **Desventajas**: Requiere manejar la lógica de streaming en tu front-end.\n",
        "- **Ideal para**: Chatbots e interfaces conversacionales donde quieres mostrar las respuestas a medida que se generan.  \n",
        "\n",
        "```python\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        "):\n",
        "    print(event, end=\"\")\n",
        "```\n",
        "\n",
        "##### 3. Webhooks (más avanzado)\n",
        "\n",
        "- **Funcionamiento**: Envías una solicitud y proporcionas una URL de callback donde recibirás la respuesta cuando esté lista.\n",
        "- **Ventajas**: Totalmente asíncrono, no bloquea recursos mientras espera, ideal para arquitecturas serverless.\n",
        "- **Desventajas**: Requiere configurar un endpoint accesible públicamente para recibir las respuestas.\n",
        "- **Ideal para**: Aplicaciones de alto rendimiento, procesamientos largos, o arquitecturas orientadas a eventos.\n",
        "\n",
        "```python\n",
        "prediction = replicate.predictions.create(\n",
        "    version=\"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"},\n",
        "    webhook=\"https://tu-aplicacion.com/webhook\"\n",
        ")\n",
        "```\n",
        "\n",
        "La elección entre estos métodos dependerá de tus necesidades y la arquitectura de tu aplicación.  \n",
        "\n",
        "A continuación tienes un ejemplo de uso de los métodos `run` y `stream`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "When we talk about \"parameters\" in the context of machine learning or deep learning, we're referring to the number of adjustable values in a model that are learned during training. Think of them like knobs or sliders that control the behavior of the model.\n",
            "\n",
            "In this case, Johnny's model has 8 billion parameters, and Tommy's model has 70 billion parameters. This doesn't directly translate to speed, but it can have an impact on the model's performance and training time.\n",
            "\n",
            "Here are a few ways in which the number of parameters can affect the model's speed:\n",
            "\n",
            "1. **Training time**: A model with more parameters will typically take longer to train, as there are more values to adjust during the learning process. Tommy's model, with 70 billion parameters, will likely take significantly longer to train than Johnny's model with 8 billion parameters.\n",
            "2. **Inference time**: During inference, the model needs to make predictions based on the input data. A model with more parameters may require more computations to produce a single output, which can lead to slower inference times. However, this effect is often mitigated by the use of optimized libraries and hardware accelerators.\n",
            "3. **Memory usage**: Models with more parameters require more memory to store the learned values. This can lead to increased memory usage, which can impact the model's performance and scalability.\n",
            "4. **Complexity**: A model with more parameters is often more complex and may require more computational resources to evaluate. This can lead to training slower and inference times, as well as increased energy consumption.\n",
            "\n",
            "In summary, while the number of parameters doesn't directly affect the model's speed, it can have indirect impacts on training time, inference time, memory usage, and complexity.\n"
          ]
        }
      ],
      "source": [
        "# Método run\n",
        "prompt =  \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. \\\n",
        "    What does this mean when it comes to speed?\"\n",
        "\n",
        "input = {\"prompt\": prompt}\n",
        "\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        ")\n",
        "\n",
        "print(\"\".join(output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "When we talk about the number of parameters in a machine learning model, it's often related to the model's complexity, capacity, and ultimately, its performance.\n",
            "\n",
            "In this case, Johnny's model has 8 billion parameters, while Tommy's model has 70 billion parameters. This means that Tommy's model has roughly 9 times more parameters than Johnny's model.\n",
            "\n",
            "Having more parameters typically allows a model to learn more complex patterns and relationships in the data, but it also increases the risk of overfitting, which can negatively impact the model's performance on new, unseen data.\n",
            "\n",
            "When it comes to speed, more parameters can also mean more computations are required to process each input, which can slow down the model's inference time. This is because the model needs to perform more calculations to generate predictions.\n",
            "\n",
            "However, it's worth noting that modern deep learning frameworks and hardware have made significant strides in optimizing model computations, so the impact of more parameters on speed may be less pronounced than it used to be.\n",
            "\n",
            "To give you a rough idea, here are some possible implications of having more parameters on speed:\n",
            "\n",
            "* If Johnny's model is a simple neural network with a few layers, having 8 billion parameters might not significantly impact its inference speed.\n",
            "* If Tommy's model is a more complex neural network with many layers, having 70 billion parameters might slow down its inference time by a factor of 2-5 compared to Johnny's model.\n",
            "* If both models are large and complex, the difference in speed might be more significant, with Tommy's model being 2-10 times slower than Johnny's model.\n",
            "\n",
            "Keep in mind that these are rough estimates, and the actual impact of more parameters on speed depends on many factors, including the specific model architecture, data size, and hardware used."
          ]
        }
      ],
      "source": [
        "# Método stream\n",
        "prompt =  \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. \\\n",
        "    What does this mean when it comes to speed?\"\n",
        "\n",
        "input = {\"prompt\": prompt}\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        "):\n",
        "    print(event, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Paso 5: Ajustar el System Prompt (y otros parámetros de configuración) según nuestras necesidades**\n",
        "\n",
        "Ahora que sabemos ejecutar el modelo de manera sencilla, debes saber que el modelo acepta muchos más parámetros además del prompt. \n",
        "\n",
        "Los diferentes parámetros ofrecen control sobre aspectos como la aleatoriedad, longitud, formato de las respuestas, etc. y te permiten ajustar con precisión el comportamiento del mismo y las características de la salida generada.\n",
        "\n",
        "Te invitamos a que explores los parámetros disponibles en la documentación del modelo en [replicate.com/meta/meta-llama-3-8b-instruct/api/schema](https://replicate.com/meta/meta-llama-3-8b-instruct/api/schema)\n",
        "\n",
        "Si bien es importante la selección adecuada de todos los parámetros (aunque la mayoría de modelos ofrecen una configuración por defecto muy equilibrada), es el ajuste del **System Prompt** lo que te permitirá adaptar la personalidad o el comportamiento del modelo a tus necesidades específicas.  \n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "¡Hola! No, no hablo inglés, pero puedo ayudarte en español. La Ciencia de Datos es un campo emocionante y en constante evolución, y Python es un lenguaje de programación muy popular y versátil para este campo. ¡Es un buen inicio!\n",
            "\n",
            "Si estás interesado en aprender a programar en Python y Ciencia de Datos, te recomiendo que investigues sobre los bootcamp de Ciencia de Datos que ofrecen instituciones como The Bridge. ¡Son muy reputados y tienen un enfoque práctico y centrado en el mercado laboral!\n",
            "\n",
            "¿Quieres saber más sobre cómo empezar a aprender Python y Ciencia de Datos?"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful and friendly assistant, who only answers in Spanish \\\n",
        "    and is very subtle recommending the Data Science Bootcamps of The Bridge, which are very good indeed.\"\n",
        "\n",
        "prompt = \"Parece que la Ciencia de Datos está muy de moda y me gustaría aprender a programar en Python, \\\n",
        "    ¿podrías darme tu opinión en inglés?\"\n",
        "\n",
        "input={\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 512,\n",
        "    \"system_prompt\": system_prompt\n",
        "    }\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input,\n",
        "):\n",
        "    print(str(event), end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En general podemos asumir que si un modelo no nos ofrece la posibilidad de editar el System Prompt se debe a que ya dispone de un System Prompt propietario que no podemos ver facilmente.\n",
        "\n",
        "Por ejemplo, OpenAI se ha ocupado de que cuando usas ChatGPT todos los días se incluya un System Prompt propietario en tus conversaciones para guiar el modelo. Es por esto que ChatGPT siempre tiende a ser educado, estructurado y a rechazar ciertos tipos de peticiones, independientemente de cómo le hables.\n",
        "\n",
        "Estos System Prompts propietarios suelen incluir instrucciones sobre tono, estructura de respuestas, límites éticos, y formato de salida.\n",
        "\n",
        "En algunos casos puedes intentar hacer un \"override\", pero los resultados varían y en general no es aconsejable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **¡Genial!** Todo listo para pasar a la segunda parte del taller\n",
        "\n",
        "Ahora que conoces los conceptos básicos del uso de modelos por API en Replicate, es el momento de que hagas tus propias pruebas y experimentes.  \n",
        "\n",
        "Replicate ofrece un extenso catálogo de modelos de IA más allá de Llama 3. Te animamos a explorar libremente la plataforma para descubrir:\n",
        "\n",
        "- Modelos de generación de imágenes (como Stable Diffusion o Flux)\n",
        "- Modelos de audio y voz\n",
        "- Modelos de visión por computadora\n",
        "- Y muchos otros casos de uso especializados\n",
        "\n",
        "Cada modelo en Replicate cuenta con su propia documentación, ejemplos y playground para experimentar antes de integrarlo en tus aplicaciones.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
